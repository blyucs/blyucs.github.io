2020.09.10

1. proxylessnas 只提供了imagenet 的训练，search 代码， cifar10 只提供了模型和eval 代码。 
2. 如果直接使用imagenet train ， 用代码提供的模型需要55 天（2块GPU）， search 过程也能正常的跑， 但是应该会更慢。 
3. 下载了tiny imagenet  :  Tinyimagenet共200个类，每个类有500个训练样本，50个验证样本，50个测试样本，由于这个是目前还在持续的挑战赛，因此测试样本不提供标签，每个样本大小是3*64*64。  其中也标注了box ，  暂未实验 。 
4. 直接使用HAQ 中用于finetune 的imagenet 100 ， 直接用符号链接，挑出了100 个类别。 每个类别训练有 1300 张， val 50 张来训练。 

关于HAQ ：

基于DDPG的混合精度量化， 逐层进行 。 





2020.09.14：

to do list ：

1. 板间数据通道建立
2. 在任意模型上实验，再次确认是否训练（例如初始化的参数）对推理时延是否有影响。  基于这个前提， 我们就可以只传架构参数给单板， 不传模型。
3. proxylessnas 服务器环境配置， 基于imagenet 200，在服务器上跑通 inference ，train and search ，熟悉代码， 选择强化学习路线或可微路线。 
4. proxyless 单板环境配置， 在单板上的inference 跑通， 考虑如何在单板复现模型进行推理。
5. 进行基于单板的多目标搜索实验，训练架构获得结果。 